{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2da8fdb5",
   "metadata": {},
   "source": [
    "## Example of AutoModelClass with Different heads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576a7d6",
   "metadata": {},
   "source": [
    "| Task                        | Class Name                                          | Description                                          |\n",
    "| --------------------------- | --------------------------------------------------- | ---------------------------------------------------- |\n",
    "| Base Model               | `AutoModel`                                         | Just embeddings (no head), for feature extraction    |\n",
    "| Masked LM                | `AutoModelForMaskedLM`                              | For BERT-style `[MASK]` prediction                   |\n",
    "| Sequence Classification | `AutoModelForSequenceClassification`                | For sentence-level tasks like sentiment, spam        |\n",
    "| Token Classification     | `AutoModelForTokenClassification`                   | For NER, POS tagging (token-level labels)            |\n",
    "| Multiple Choice          | `AutoModelForMultipleChoice`                        | For MCQ tasks (e.g., SWAG dataset)                   |\n",
    "| Next Sentence Prediction | `AutoModelForNextSentencePrediction`                | For NSP (mainly BERT pretraining)                    |\n",
    "| Causal LM (Text Gen)     | `AutoModelForCausalLM`                              | For GPT-style models (left-to-right text generation) |\n",
    "| Seq2Seq LM               | `AutoModelForSeq2SeqLM`                             | For translation, summarization (T5, BART)            |\n",
    "| Question Answering       | `AutoModelForQuestionAnswering`                     | For extractive QA (SQuAD-style)                      |\n",
    "| Vision Tasks            | `AutoModelForImageClassification`, etc.             | For image classification (e.g., ViT)                 |\n",
    "| Audio Tasks              | `AutoModelForAudioClassification`, etc.             | For Wav2Vec, Whisper, etc.                           |\n",
    "| Conditional Generation   | `AutoModelForConditionalGeneration`                 | For T5/BART-style seq2seq tasks                      |\n",
    "| Zero-shot Tasks          | `AutoModelForZeroShotClassification` (via pipeline) | For inference without fine-tuning                    |\n",
    "| Contrastive Learning     | `AutoModelForContrastiveLearning`                   | For embeddings comparison tasks                      |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73cd8ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba1679a5",
   "metadata": {},
   "source": [
    "## Inbuilt Pipeline method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d496b539",
   "metadata": {},
   "source": [
    "| Parameter           | Type                        | Description / Use Case                                                               |\n",
    "| ------------------- | --------------------------- | ------------------------------------------------------------------------------------ |\n",
    "| `task`              | `str`                       | Task name like `\"text-classification\"`, `\"text-generation\"`, `\"summarization\"`, etc. |\n",
    "| `model`             | `str` or `PreTrainedModel`  | Model name (e.g., `\"bert-base-uncased\"`) or loaded model object                      |\n",
    "| `tokenizer`         | `str` or Tokenizer Object   | Optional – auto-loaded from model if not given                                       |\n",
    "| `config`            | `str` or `PretrainedConfig` | Optional – custom configuration (e.g., num labels, dropout, etc.)                    |\n",
    "| `framework`         | `str` (`\"pt\"` or `\"tf\"`)    | Force PyTorch or TensorFlow (auto-detected if not set)                               |\n",
    "| `device`            | `int` or `str`              | `0` = GPU, `-1` = CPU, `\"cuda\"` or `\"cpu\"`                                           |\n",
    "| `revision`          | `str`                       | Git branch/tag/commit from Hugging Face Hub (e.g., `\"main\"`, `\"v1.0\"`)               |\n",
    "| `use_fast`          | `bool`                      | Whether to use fast tokenizer (defaults to `True`)                                   |\n",
    "| `token`             | `str` or `bool`             | Hugging Face token (needed for private models or rate limits)                        |\n",
    "| `device_map`        | `Any`                       | For multi-GPU / model parallelism                                                    |\n",
    "| `torch_dtype`       | `torch.dtype`               | Force precision: e.g., `torch.float16` for faster inference                          |\n",
    "| `trust_remote_code` | `bool`                      | Allow loading custom `model.py` logic from HF repo (for advanced/custom models)      |\n",
    "| `model_kwargs`      | `dict`                      | Extra arguments to pass to model (e.g., `temperature`, `top_p`, `max_length`, etc.)  |\n",
    "| `feature_extractor` | `str` or Object             | Used for older vision/audio models (now replaced by `image_processor`)               |\n",
    "| `image_processor`   | `BaseImageProcessor`        | For image tasks (used in ViT, SAM, etc.)                                             |\n",
    "| `processor`         | `ProcessorMixin`            | For multi-modal models (e.g., Whisper, CLIP, LayoutLMv3)                             |\n",
    "| `pipeline_class`    | `Any`                       | Custom pipeline class if you’re extending or modifying the base pipeline logic       |\n",
    "| `**kwargs`          | `Any`                       | Additional pipeline-specific keyword arguments (like `max_length`, `top_k`, etc.)    |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8327f509",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37bc6152",
   "metadata": {},
   "source": [
    "## BLEU vs ROUGE: Evaluation Metrics Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ecdbc5",
   "metadata": {},
   "source": [
    "| Feature                 | **BLEU (Bilingual Evaluation Understudy)**          | **ROUGE (Recall-Oriented Understudy for Gisting Evaluation)** |\n",
    "| ----------------------- | --------------------------------------------------- | ------------------------------------------------------------- |\n",
    "| **Primary Focus**       | Precision (How much predicted is correct)           | Recall (How much reference is captured)                       |\n",
    "| **N-gram type**         | 1-gram to 4-gram precision                          | 1-gram to 4-gram recall + LCS (ROUGE-L)                       |\n",
    "| **Formula Style**       | BLEU = BP × exp(avg log precision)                  | ROUGE = overlap / reference length                            |\n",
    "| **Brevity Penalty**     | Yes — penalizes too-short output                    | No — doesn’t punish short outputs                             |\n",
    "| **Best For**            | Machine Translation, factual generation             | Summarization, content preservation                           |\n",
    "| **Fails When**          | Synonyms used, multiple valid outputs               | Extra irrelevant content is present                           |\n",
    "| **Overlap type**        | Exact n-gram match (no flexibility)                 | Longest sequence, flexible phrasing                           |\n",
    "| **Multiple References** | Supported                                           | Supported                                                     |\n",
    "| **Interpretation**      | Higher BLEU = more precise and fluent output        | Higher ROUGE = more informative and complete output           |\n",
    "| **Used In**             | Machine Translation, chatbot QA, factual generation | Summarization, headline generation                            |\n",
    "| **Metric Values**       | 0 to 1 (or 0 to 100%)                               | 0 to 1 (or 0 to 100%)                                         |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c5adb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "599df54e",
   "metadata": {},
   "source": [
    "## Decision Tree: Which AutoModel Should I Pick?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06547059",
   "metadata": {},
   "source": [
    "```\n",
    "START\n",
    " |\n",
    " |–– Do you only need embeddings / hidden states?\n",
    " |        |\n",
    " |        |–– YES → AutoModel\n",
    " |        |\n",
    " |        |–– NO\n",
    " |\n",
    " |–– Are you generating text?\n",
    " |        |\n",
    " |        |–– YES\n",
    " |        |     |\n",
    " |        |     |–– Left-to-right generation (GPT-style)?\n",
    " |        |     |        → AutoModelForCausalLM\n",
    " |        |     |\n",
    " |        |     |–– Encoder–decoder generation (translate, summarize)?\n",
    " |        |              → AutoModelForSeq2SeqLM\n",
    " |        |\n",
    " |        |–– NO\n",
    " |\n",
    " |–– Are you predicting labels?\n",
    " |        |\n",
    " |        |–– Sentence-level label?\n",
    " |        |        → AutoModelForSequenceClassification\n",
    " |        |\n",
    " |        |–– Token-level label?\n",
    " |        |        → AutoModelForTokenClassification\n",
    " |        |\n",
    " |        |–– Start–end span in text?\n",
    " |        |        → AutoModelForQuestionAnswering\n",
    " |        |\n",
    " |        |–– Multiple options per example?\n",
    " |        |        → AutoModelForMultipleChoice\n",
    " |\n",
    " |–– Are you filling in [MASK] tokens?\n",
    " |        |\n",
    " |        |–– YES → AutoModelForMaskedLM\n",
    " |\n",
    " |–– Is this a vision task?\n",
    " |        |\n",
    " |        |–– Image classification?\n",
    " |        |        → AutoModelForImageClassification\n",
    " |        |\n",
    " |        |–– Object detection?\n",
    " |        |        → AutoModelForObjectDetection\n",
    " |        |\n",
    " |        |–– Image → text?\n",
    " |        |        → AutoModelForVision2Seq\n",
    " |\n",
    " |–– Is this an audio task?\n",
    " |        |\n",
    " |        |–– Speech → text?\n",
    " |        |        → AutoModelForSpeechSeq2Seq\n",
    " |        |\n",
    " |        |–– Audio classification?\n",
    " |        |        → AutoModelForAudioClassification\n",
    " |\n",
    " |–– Is it multimodal (text + image/audio)?\n",
    " |        |\n",
    " |        |–– Generation involved?\n",
    " |        |        → AutoModelForVision2Seq / AutoModelForSeq2SeqLM\n",
    " |\n",
    " |–– Are you doing similarity / retrieval?\n",
    " |        |\n",
    " |        |–– Bi-encoder / embeddings?\n",
    " |        |        → AutoModel\n",
    " |        |\n",
    " |        |–– Cross-encoder reranking?\n",
    " |        |        → AutoModelForSequenceClassification\n",
    " |\n",
    " END\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce375c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
