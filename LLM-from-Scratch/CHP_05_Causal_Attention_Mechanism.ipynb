{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "CLRcYG3X7Kg3"
      ],
      "authorship_tag": "ABX9TyPeCY+VDBqBxIzgjheLuZ+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArpitKadam/Attention-Is-All-You-Code/blob/main/LLM-from-Scratch/CHP_05_Causal_Attention_Mechanism.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAUSAL ATTENTION IMPLEMENTATION**"
      ],
      "metadata": {
        "id": "CLRcYG3X7Kg3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FPDm8rVWmxHw"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "input = torch.tensor(\n",
        "    [[0.72, 0.45, 0.31],   ## Dream\n",
        "     [0.75, 0.20, 0.55],   ## big\n",
        "     [0.30, 0.80, 0.40],   ## and\n",
        "     [0.85, 0.35, 0.60],   ## work\n",
        "     [0.55, 0.15, 0.75],   ## for\n",
        "     [0.20, 0.20, 0.85]]   ## it\n",
        ")\n",
        "\n",
        "words = [\"Dream\", \"big\", \"and\", \"work\", \"for\", \"it\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v2(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.T)\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** (0.5)), dim=-1)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "QpZyYNrPuwgY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "\n",
        "sa_v2 = SelfAttention_v2(3, 3, True)\n",
        "queries = sa_v2.W_query(input)\n",
        "keys = sa_v2.W_key(input)\n",
        "values = sa_v2.W_value(input)\n",
        "\n",
        "attn_scores = torch.matmul(queries, keys.T)\n",
        "attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** (0.5)), dim=-1)\n",
        "\n",
        "print(\"Attention Scores:\")\n",
        "print(attn_scores)\n",
        "print(\"Shape:\", attn_scores.shape)\n",
        "print()\n",
        "print(\"Attention Weights:\")\n",
        "print(attn_weights)\n",
        "print(\"Shape:\", attn_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V83TTYSMvRcR",
        "outputId": "58ef8790-4908-4e32-af28-4216693b7f02"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:\n",
            "tensor([[-0.6868, -0.5927, -0.7045, -0.6571, -0.5185, -0.4539],\n",
            "        [-0.7825, -0.6745, -0.7996, -0.7516, -0.5871, -0.5083],\n",
            "        [-0.4230, -0.3595, -0.4378, -0.3796, -0.3166, -0.2920],\n",
            "        [-0.7654, -0.6544, -0.7836, -0.7150, -0.5695, -0.5027],\n",
            "        [-0.7309, -0.6282, -0.7459, -0.6976, -0.5454, -0.4725],\n",
            "        [-0.5937, -0.5097, -0.6058, -0.5651, -0.4422, -0.3834]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "Shape: torch.Size([6, 6])\n",
            "\n",
            "Attention Weights:\n",
            "tensor([[0.1585, 0.1674, 0.1569, 0.1612, 0.1747, 0.1813],\n",
            "        [0.1571, 0.1673, 0.1556, 0.1600, 0.1759, 0.1841],\n",
            "        [0.1614, 0.1674, 0.1600, 0.1655, 0.1716, 0.1741],\n",
            "        [0.1570, 0.1674, 0.1554, 0.1617, 0.1758, 0.1827],\n",
            "        [0.1576, 0.1672, 0.1562, 0.1606, 0.1754, 0.1829],\n",
            "        [0.1592, 0.1672, 0.1581, 0.1619, 0.1738, 0.1798]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Shape: torch.Size([6, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0D4hBsHM1OEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "simple_mask = torch.tril(torch.ones(6, 6))\n",
        "print(simple_mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbKhnhzAw8mP",
        "outputId": "062dc8af-0430-43e0-b15a-ae537333b4ef"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple = attn_weights * simple_mask\n",
        "print(masked_simple)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2lL_R2-xKL0",
        "outputId": "8faf8576-2612-4299-aec9-ce58c0733258"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1585, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1571, 0.1673, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1614, 0.1674, 0.1600, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1570, 0.1674, 0.1554, 0.1617, 0.0000, 0.0000],\n",
            "        [0.1576, 0.1672, 0.1562, 0.1606, 0.1754, 0.0000],\n",
            "        [0.1592, 0.1672, 0.1581, 0.1619, 0.1738, 0.1798]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "row_sum = masked_simple.sum(dim=1, keepdim=True)\n",
        "print(row_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZX8Y8T1w66b",
        "outputId": "97acb876-5ab2-4811-dbca-ab6d0a02b040"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.1585],\n",
            "        [0.3244],\n",
            "        [0.4888],\n",
            "        [0.6415],\n",
            "        [0.8171],\n",
            "        [1.0000]], grad_fn=<SumBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked_simple_norm = masked_simple / row_sum\n",
        "print(masked_simple_norm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sz31szdYxrnR",
        "outputId": "d07f631c-6431-46c7-d078-ed61d6f921e5"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4844, 0.5156, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3302, 0.3425, 0.3273, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2448, 0.2610, 0.2422, 0.2520, 0.0000, 0.0000],\n",
            "        [0.1929, 0.2047, 0.1912, 0.1966, 0.2147, 0.0000],\n",
            "        [0.1592, 0.1672, 0.1581, 0.1619, 0.1738, 0.1798]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CuIALdJ9x4fn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Upper Method leads to Data Leakage Problems, since we are applying Softmax before normalizing and converting to lower triangular matrix"
      ],
      "metadata": {
        "id": "YcG_0dejyL9U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we apply a more efficient method\n",
        "\n",
        "- Attention Score -> Upper Triangular Matrix -> Softmax (Attention Weights)"
      ],
      "metadata": {
        "id": "zTlWLb9oy9Ip"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aXJ0gf3Qxish"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Attention Scores:\")\n",
        "print(attn_scores)\n",
        "print(\"Shape:\", attn_scores.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXuO-otxxhKJ",
        "outputId": "1dac7b2f-b22e-4f7a-9750-1e969f45ff73"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention Scores:\n",
            "tensor([[-0.6868, -0.5927, -0.7045, -0.6571, -0.5185, -0.4539],\n",
            "        [-0.7825, -0.6745, -0.7996, -0.7516, -0.5871, -0.5083],\n",
            "        [-0.4230, -0.3595, -0.4378, -0.3796, -0.3166, -0.2920],\n",
            "        [-0.7654, -0.6544, -0.7836, -0.7150, -0.5695, -0.5027],\n",
            "        [-0.7309, -0.6282, -0.7459, -0.6976, -0.5454, -0.4725],\n",
            "        [-0.5937, -0.5097, -0.6058, -0.5651, -0.4422, -0.3834]],\n",
            "       grad_fn=<MmBackward0>)\n",
            "Shape: torch.Size([6, 6])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.triu(torch.ones(6,6), diagonal=1)\n",
        "print(mask)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aet4kAWr1p0d",
        "outputId": "1bd22457-efee-4323-8cd6-cec3afabee01"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 1., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 0., 1.],\n",
            "        [0., 0., 0., 0., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask.bool()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AwPq9yp1rJI",
        "outputId": "87f30d7c-d909-475b-b6a2-7d28a34206fc"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True,  True,  True,  True,  True],\n",
              "        [False, False,  True,  True,  True,  True],\n",
              "        [False, False, False,  True,  True,  True],\n",
              "        [False, False, False, False,  True,  True],\n",
              "        [False, False, False, False, False,  True],\n",
              "        [False, False, False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "masked = attn_scores.masked_fill(mask.bool(), -torch.inf)\n",
        "print(masked)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzukRCG8zfwP",
        "outputId": "8b3a004e-173d-48c7-aae2-c092851e1720"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.6868,    -inf,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.7825, -0.6745,    -inf,    -inf,    -inf,    -inf],\n",
            "        [-0.4230, -0.3595, -0.4378,    -inf,    -inf,    -inf],\n",
            "        [-0.7654, -0.6544, -0.7836, -0.7150,    -inf,    -inf],\n",
            "        [-0.7309, -0.6282, -0.7459, -0.6976, -0.5454,    -inf],\n",
            "        [-0.5937, -0.5097, -0.6058, -0.5651, -0.4422, -0.3834]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(masked / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSNEUE0w2CJ2",
        "outputId": "dcafa727-a7c2-4d1f-bd03-ebd91f0c0dc2"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4844, 0.5156, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3302, 0.3425, 0.3273, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2448, 0.2610, 0.2422, 0.2520, 0.0000, 0.0000],\n",
            "        [0.1929, 0.2047, 0.1912, 0.1966, 0.2147, 0.0000],\n",
            "        [0.1592, 0.1672, 0.1581, 0.1619, 0.1738, 0.1798]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UvzPjsOb2AwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Masking Additional Attention Weights with Dropout**"
      ],
      "metadata": {
        "id": "FuVZufpw5fuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "example = torch.ones(6, 6)\n",
        "print(dropout(example))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aA9Ek3y21WPl",
        "outputId": "5b4e8caa-d758-4399-8f3e-9b7828277258"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., 2., 2., 2., 0., 0.],\n",
            "        [0., 2., 0., 0., 2., 2.],\n",
            "        [0., 0., 0., 2., 2., 2.],\n",
            "        [0., 2., 2., 2., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [2., 0., 2., 2., 0., 0.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(100)\n",
        "dropout = torch.nn.Dropout(0.5)\n",
        "print(dropout(attn_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8hKwa5R6e5R",
        "outputId": "d857a1c6-7b86-4716-fb2f-176ad7441f9c"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 1.0312, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.5220, 0.4844, 0.5040, 0.0000, 0.0000],\n",
            "        [0.3857, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3185, 0.0000, 0.3163, 0.3238, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0Gnitx763mu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **CAUSAL ATTENTION CLASS**"
      ],
      "metadata": {
        "id": "3cOlOhs07VkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, context_length, dropout, qkv_bias):\n",
        "    super().__init__()\n",
        "    self.d_out = d_out\n",
        "    self.context_length = context_length\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.register_buffer(\"simple_mask\", torch.tril(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    batch_size, num_tokens, d_in = x.shape\n",
        "\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = torch.matmul(queries, keys.transpose(-2, -1))\n",
        "\n",
        "    attn_scores.masked_fill_(\n",
        "        self.simple_mask.bool()[:num_tokens, :num_tokens],\n",
        "        -torch.inf\n",
        "    )\n",
        "\n",
        "    attn_weights = torch.softmax(attn_scores / (keys.shape[-1] ** 0.5), dim=-1)\n",
        "\n",
        "    attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "    context_vec = torch.matmul(attn_weights, values)\n",
        "\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "P9L84cHB6LCt"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack([inputs, inputs], dim=0)\n",
        "print(batch.shape)\n",
        "\n",
        "d_in = batch.shape[-1]\n",
        "d_out = 2\n",
        "context_length = batch.shape[1]\n",
        "dropout = 0.0\n",
        "qkv_bias = False"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xenxBisLA8xi",
        "outputId": "572bfe93-7312-4ec1-ff35-7afc8788260c"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "causal_attn = CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)\n",
        "context_vector = causal_attn(batch)"
      ],
      "metadata": {
        "id": "Zq7SQ6_N_2e6"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Context Vector:\")\n",
        "print(context_vector)\n",
        "print(\"Shape:\", context_vector.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMRMD8ihAStB",
        "outputId": "2f9a004f-e9c1-410a-b86d-fee541a49b01"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context Vector:\n",
            "tensor([[[-0.5045, -0.1662],\n",
            "         [-0.4305, -0.1552],\n",
            "         [-0.4545, -0.1550],\n",
            "         [-0.4213, -0.1501],\n",
            "         [    nan,     nan],\n",
            "         [    nan,     nan]],\n",
            "\n",
            "        [[-0.5045, -0.1662],\n",
            "         [-0.4305, -0.1552],\n",
            "         [-0.4545, -0.1550],\n",
            "         [-0.4213, -0.1501],\n",
            "         [    nan,     nan],\n",
            "         [    nan,     nan]]], grad_fn=<UnsafeViewBackward0>)\n",
            "Shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6OkllKRK1VAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The use of register_buffer in\n",
        "PyTorch is not strictly necessary for all use cases but offers several advantages here.\n",
        "\n",
        "For\n",
        "instance, when we use the CausalAttention class in our LLM, buffers are automatically\n",
        "moved to the appropriate device (CPU or GPU) along with our model, which will be relevant\n",
        "when training the LLM in future chapters.\n",
        "\n",
        "This means we don't need to manually ensure\n",
        "these tensors are on the same device as your model parameters, avoiding device mismatch\n",
        "errors."
      ],
      "metadata": {
        "id": "98PS5mazCcRt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wtyZThpp0EVa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}